{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vf_portalytics.transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-150f65fd983b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvf_portalytics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpotential_transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvf_portalytics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouped_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGrouped_Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vf_portalytics.transformers'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost \n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n",
    "from functools import partial\n",
    "from vf_portalytics.tool import squared_error_objective_with_weighting\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFE\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from vf_portalytics.transformers import OneHotEncoder, potential_transformers\n",
    "from vf_portalytics.grouped_model import Grouped_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_dataset(n_samples, n_features, n_informative, **kwargs):\n",
    "    x, y = make_regression(\n",
    "        n_samples=n_samples, \n",
    "        n_features=n_features,\n",
    "        noise=0.5,\n",
    "        n_informative=n_informative, \n",
    "        random_state=0\n",
    "    )\n",
    "    x = pd.DataFrame(x)\n",
    "    \n",
    "    x.columns = ['feature_' + str(i) for i in range(n_features)]\n",
    "    x = x.assign(**kwargs)\n",
    "    return x, pd.Series(y, name='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate data for 4 different categories\n",
    "# different #samples for each category but the same #features since they belong to the same dataset\n",
    "n_features = 20\n",
    "x1, y1 = make_dataset(n_samples=100, n_features=n_features, n_informative=5, category='A')\n",
    "x2, y2 = make_dataset(n_samples=150, n_features=n_features, n_informative=8, category='B')\n",
    "x3, y3 = make_dataset(n_samples=80, n_features=n_features, n_informative=7, category='C')\n",
    "x4, y4 = make_dataset(n_samples=120, n_features=n_features, n_informative=6, category='D')\n",
    "\n",
    "# combine into one dataset\n",
    "total_x = pd.concat([x1, x2, x3, x4], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "total_y = pd.concat([y1, y2, y3, y4], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "# make two random features categorical\n",
    "labels = [0, 1, 2]\n",
    "bins = [[],[]]\n",
    "for i in range(2):\n",
    "    bins[i] = [-np.inf, \n",
    "               total_x['feature_' + str(i)].mean() - total_x['feature_' + str(i)].std(), \n",
    "               total_x['feature_' + str(i)].mean() + total_x['feature_' + str(i)].std(), \n",
    "               total_x['feature_' + str(i)].max()]\n",
    "total_x['feature_0'] = pd.cut(total_x['feature_0'], bins=bins[0], labels=labels)\n",
    "total_x['feature_1'] = pd.cut(total_x['feature_1'], bins=bins[1], labels=labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare group parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Declare basic parameters\n",
    "target = 'target'\n",
    "cat_feature = 'category'\n",
    "feature_col_list = total_x.columns.drop(cat_feature)\n",
    "\n",
    "clusters = total_x[cat_feature].unique()\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_index, test_index = train_test_split(total_x.index, test_size=0.33, random_state=5)\n",
    "train_x, train_y = total_x.loc[train_index, :], total_y.loc[train_index]\n",
    "test_x, test_y = total_x.loc[test_index, :], total_y.loc[test_index]\n",
    "\n",
    "del x1, x2, x3, x4\n",
    "del y1, y2, y3, y4\n",
    "del total_x, total_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def feature_selection(df_X, df_Y, features_len=1, step=1, max_features=20):\n",
    "\n",
    "    if features_len > max_features:\n",
    "        features_len = max_features\n",
    "        \n",
    "    # we fit with a lighter but representative model\n",
    "    model=xgboost.XGBRegressor(\n",
    "        max_depth=5,\n",
    "        n_estimators=50,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        n_jobs=6,\n",
    "        objective = partial(squared_error_objective_with_weighting, under_predict_weight=2.0), \n",
    "        seed=6789,\n",
    "        silent=True\n",
    "    )\n",
    "\n",
    "    # create the RFE model and select the attributes\n",
    "    rfe = RFE(model,  n_features_to_select=features_len, step=step)\n",
    "    rfe = rfe.fit(df_X, df_Y)\n",
    "    return rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set your parameters; please be aware that mutual_info_regression can be very resource intensive\n",
    "max_features = train_x.shape[1]\n",
    "features_len = 1  # max number of columns: 'all' or a number\n",
    "step = 1  # x features to be dropped each step\n",
    "\n",
    "groups = train_x.groupby(cat_feature)\n",
    "feature_importances = {}\n",
    "for gp_key, x_group in groups:\n",
    "    print('Searching for Feature Ranking in ' + gp_key + '...')\n",
    "    x_group = x_group.drop(cat_feature, axis=1)\n",
    "    y_group = train_y.loc[x_group.index]\n",
    "    # find best parameters for each model-group\n",
    "    feature_importances[gp_key] = feature_selection(x_group, y_group, \n",
    "                                                    features_len=1, step=1, max_features=max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check The Ranking and manually decide which Features to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The features ordered by importance for each group: \\n')\n",
    "ordered_feautures = []\n",
    "for cluster in clusters:\n",
    "    elements = sorted(list(zip(feature_importances[cluster].ranking_, feature_col_list)))\n",
    "    ordered_feautures.append(list(zip(*elements))[1])\n",
    "ordered_feautures_df = pd.DataFrame(ordered_feautures,  index=clusters)\n",
    "ordered_feautures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Manually Select features\n",
    "# default select 10 most important\n",
    "selected_features = {}\n",
    "for group_key, _ in  ordered_feautures_df.iterrows():  \n",
    "    selected_features[group_key] = list(ordered_feautures_df.loc[group_key, 0:10].values)\n",
    "\n",
    "# change mannually the features\n",
    "\n",
    "# Discard features that are not going to be in the future\n",
    "# And discard features that are not important from the \"business perspective\"\n",
    "    \n",
    "# In the end selected_features; a dictionary with keys the group names and values list of features that are going to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# space can be different for each group but let this for the future if it is needed\n",
    "space={\n",
    "    'n_estimators' : hp.choice('n_estimators', np.arange(100, 500, 50, dtype=int)),\n",
    "    'max_depth': hp.choice('max_depth', np.arange(1, 10, dtype = int)),\n",
    "    'subsample': hp.quniform('subsample', 0.8, 1.0, 0.05),\n",
    "    'min_child_weight': hp.quniform ('min_child_weight', 1, 20, 1),\n",
    "    'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "    'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05), \n",
    "    'learning_rate' : hp.quniform('learning_rate', 0.025, 0.5, 0.025),\n",
    "    'transformer': hp.choice('transformer', ['OneHotEncoder']),\n",
    "}\n",
    "\n",
    "def score(params):\n",
    "    num_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "#     x, y = preprocess(x, y, params['transformer'])\n",
    "\n",
    "    # preprocess (the transformation is being done in the cross_val)\n",
    "    transformer = potential_transformers.get(params['transformer'])\n",
    "\n",
    "    gbm_model = xgboost.XGBRegressor(n_estimators = num_round, \n",
    "                                     objective = partial(squared_error_objective_with_weighting, under_predict_weight=2.0), \n",
    "                                     max_depth = params['max_depth'],\n",
    "                                     subsample = params['subsample'],\n",
    "                                     min_child_weight = params['min_child_weight'],\n",
    "                                     gamma = params['gamma'],\n",
    "                                     colsample_bytree = params['colsample_bytree'],\n",
    "                                     learning_rate = params['learning_rate'],\n",
    "                                     n_jobs = 8,\n",
    "                                     seed = 1234,\n",
    "                                     silent=True)\n",
    "    \n",
    "    pipeline = Pipeline([('transformer', transformer), ('estimator', gbm_model)])\n",
    "\n",
    "    score = cross_val_score(pipeline, x_group, y_group, cv=KFold(n_splits=5, random_state=9876), \n",
    "                            scoring='neg_mean_squared_error')\n",
    "    loss = np.abs(np.mean(score))\n",
    "    return {'loss' : loss, 'status' : STATUS_OK}\n",
    "\n",
    "\n",
    "def optimize(space, x_group, y_group, gp_key):\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=score, \n",
    "                space=space, \n",
    "                algo=tpe.suggest, \n",
    "                max_evals=20, #What value is optimal?\n",
    "                trials=trials\n",
    "               )\n",
    "    return space_eval(space, best), trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "groups = train_x.groupby(cat_feature)\n",
    "params = {}\n",
    "for gp_key, group in groups:\n",
    "    print('Checking ' + gp_key + ' ...')\n",
    "    # keep only the most improtant features\n",
    "    x_group = group[selected_features[gp_key]]\n",
    "    y_group = train_y[x_group.index]\n",
    "    # find best parameters for each model-group\n",
    "    best_params, trials = optimize(space, x_group, y_group, gp_key)\n",
    "    params[gp_key] = best_params\n",
    "    \n",
    "# in the end we keep params; a dictionary with keys the group names and values dictionaries of the selected hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params\n",
    "# Here we can specify which feature we want to consider as categoricals eg:\n",
    "# params['A']['potential_cat_feat'] = set(['feature_1', 'feature_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and model and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initiliaze model\n",
    "model = Grouped_Model(group_col=cat_feature, clusters=clusters, params=params, selected_features=selected_features)\n",
    "\n",
    "model.fit(train_x, train_y)\n",
    "pred_test_y = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'predicted': pred_test_y, 'actuals': test_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
